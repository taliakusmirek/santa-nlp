{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\n\n# Import Data\ndata = pd.read_csv(\"/kaggle/input/santa-2024/sample_submission.csv\")\n# print(data) # Correctly locates competition data\ntext_data = data['text'].tolist()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T23:43:39.199364Z","iopub.execute_input":"2024-12-27T23:43:39.199829Z","iopub.status.idle":"2024-12-27T23:43:39.217435Z","shell.execute_reply.started":"2024-12-27T23:43:39.199789Z","shell.execute_reply":"2024-12-27T23:43:39.215969Z"}},"outputs":[{"name":"stdout","text":"   id                                               text\n0   0  advent chimney elf family fireplace gingerbrea...\n1   1  advent chimney elf family fireplace gingerbrea...\n2   2  yuletide decorations gifts cheer holiday carol...\n3   3  yuletide decorations gifts cheer holiday carol...\n4   4  hohoho candle poinsettia snowglobe peppermint ...\n5   5  advent chimney elf family fireplace gingerbrea...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from nltk import bigrams\nfrom collections import Counter, defaultdict\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nimport nltk\n\n# Tokenize data to compare tokens\ntokenized_data = []\nfor sentence in text_data:\n    tokenized_data += nltk.word_tokenize(sentence)\n    \n# Create a placeholder for model\nmodel = defaultdict(lambda: defaultdict(lambda: 0))\n\nfor idx, row in data.iterrows():\n    words = row['text'].lower().split()\n    for w1, w2 in bigrams(words):\n        model[w1][w2] += 1\n\n# Calculate probability of w2 if previously at w1\nfor w1 in model:\n    total_count = float(sum(model[w1].values()))\n    for w2 in model[w1]:\n        model[w1][w2] /= total_count\n\ndef calculate_sequence_probability(sequence):\n    if len(sequence) < 2:\n        return 0\n    \n    prob = 1 # Default \n    words = sequence.lower().split()\n    for w1, w2 in bigrams(words):\n        prob *= (model[w1][w2] + 1e-10)  # Add smoothing to avoid zero probabilities\n    return prob\n\n# Idea Implementation:\n# Create a loop to compare each word one by one as a pair with every other word in the row for each row in the csv\n# Establish a min probability of 1 (just for comparison sake)\n# For each word in the list besides current word, calculate bigram_probability\n# If bigram_probability of this pair is less than min_probability of all pairs, set min_probability to this and set this word after our current word\n# Account for moving arounds values each time, and repeat\n\nresult_df = data.copy()\ndetokenizer = TreebankWordDetokenizer()\nfor idx, row in data.iterrows():\n    words = row['text'].split()\n    best_sequence = words.copy()\n    best_prob = calculate_sequence_probability(' '.join(best_sequence))\n\n    # Simple greedy optimization\n    sort = True\n    while sort:\n        sort = False\n        for i in range(len(words)):\n            for j in range(i+1, len(words)):\n                # swap our current word with each word in the list\n                test_sequence = best_sequence.copy()\n                test_sequence[i], test_sequence[j] = test_sequence[j], test_sequence[i]\n\n                # calculate probability of each specific pair, keep if better\n                test_prob = calculate_sequence_probability(' '.join(test_sequence))\n\n                if test_prob > best_prob:\n                    best_sequence = test_sequence\n                    best_prob = test_prob\n                    improved = true\n\n    \n    # and then put data text row into the same order as the best order, provided via the sorted probabilities dictionary\n    result_df.loc[idx, 'text'] = ' '.join(best_sequence)\n\n    # Put back into correct row of text column in given csv file for submission\nresult_df.to_csv('submission.csv', index=False)\n\n        \n        \n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}